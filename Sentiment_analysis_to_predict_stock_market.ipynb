{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO/o3SCiNRyrsnqH0g8DTl+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thaivo02/Sentiment-analysis/blob/main/Sentiment_analysis_to_predict_stock_market.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "ZBlpALwIuqcY"
      }
    },
    {
      "source": [
        "!pip install nltk\n",
        "!pip install underthesea\n",
        "!pip install transformers torch\n",
        "!pip install SentencePiece"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGOxhVxZ4bcO",
        "outputId": "8ba11f88-08ff-43d9-cef0-3a1609788eea",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Collecting underthesea\n",
            "  Downloading underthesea-6.8.4-py3-none-any.whl (20.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.7)\n",
            "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.31.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0.1)\n",
            "Collecting underthesea-core==1.0.4 (from underthesea)\n",
            "  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2024.5.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2024.6.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.5.0)\n",
            "Installing collected packages: underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.10 underthesea-6.8.4 underthesea-core-1.0.4\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fP0YXG05b6OA"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import math\n",
        "import string\n",
        "import re\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from matplotlib.ticker import FormatStrFormatter\n",
        "import underthesea\n",
        "from underthesea import word_tokenize\n",
        "from underthesea import text_normalize\n",
        "from datetime import datetime\n",
        "import timeit\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import RobertaForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "id": "9_TdL_xd-UUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "KUpEBrJvhG_U",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data"
      ],
      "metadata": {
        "id": "M6mYVTYiu7SB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "company_list = ['API', 'AGG', 'BID', 'FPT', 'VCB', 'ACV', 'OIL', 'ABB', 'ABC']"
      ],
      "metadata": {
        "id": "MzzTcXSCpQXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Call API to get data"
      ],
      "metadata": {
        "id": "Jff8V_uzvBh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headers={\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 OPR/109.0.0.0\"}"
      ],
      "metadata": {
        "id": "Os2C1FjFcU0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "company_news = {}"
      ],
      "metadata": {
        "id": "7dW4fBpUzOeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for company in company_list:\n",
        "    index = 1\n",
        "    finished = False\n",
        "    company_news[company] = {}\n",
        "    company_news[company]['news'] = []\n",
        "    company_news[company]['timeline'] = []\n",
        "    while not finished:\n",
        "        url = f\"https://s.cafef.vn/Ajax/Events_RelatedNews_New.aspx?symbol={company}&floorID=0&configID=0&PageIndex={index}&PageSize=30&Type=2\"\n",
        "        response = requests.get(url, headers = headers)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "            titles = soup.find_all(\"a\", class_=\"docnhanhTitle\")\n",
        "            titles = [title.text.strip().split(\": \")[-1] for title in titles]\n",
        "            timeline = soup.find_all(\"span\", class_=\"timeTitle\")\n",
        "            timeline = [time.text.strip().split(' ')[0] for time in timeline]\n",
        "            print(len(company_news[company]['news']))\n",
        "            if len(company_news[company]['news']) == 0:\n",
        "                company_news[company]['news'] = titles\n",
        "                company_news[company]['timeline'] = timeline\n",
        "            else:\n",
        "                company_news[company]['news'].extend(titles)\n",
        "                company_news[company]['timeline'].extend(timeline)\n",
        "            print(f\"Fetched data for {company}\")\n",
        "        else:\n",
        "            print(f\"Failed to fetch data for {company}\")\n",
        "        if len(company_news[company]['news']) >= 100:\n",
        "            finished = True\n",
        "        else:\n",
        "            index += 1"
      ],
      "metadata": {
        "id": "W2703k-vxVSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for company in company_list:\n",
        "    company_date = list(company_news[company]['timeline'])\n",
        "    company_news[company]['price'] = []\n",
        "    startDate = company_date[-1]\n",
        "    startDate = datetime.strptime(startDate, \"%d/%m/%Y\")\n",
        "    startDate = startDate.strftime(\"%m/%d/%Y\")\n",
        "    endDate = company_date[0]\n",
        "    endDate = datetime.strptime(endDate, \"%d/%m/%Y\")\n",
        "    endDate = endDate.strftime(\"%m/%d/%Y\")\n",
        "    finished = False\n",
        "    index = 1\n",
        "    while not finished:\n",
        "      url = f\"https://s.cafef.vn/Ajax/PageNew/DataHistory/PriceHistory.ashx?Symbol={company}&StartDate={startDate}&EndDate={endDate}&PageIndex={index}\"\n",
        "      response = requests.get(url)\n",
        "      if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        if 'Data' in data:\n",
        "            data = data['Data']['Data']\n",
        "            for date in company_date:\n",
        "                for item in data:\n",
        "                    if item['Ngay'] == date:\n",
        "                        company_news[company]['price'].append(float(item['GiaDongCua']))\n",
        "      else:\n",
        "        print(f\"Failed to fetch price data for {company}\")\n",
        "      if len(data) == 0:\n",
        "        finished = True\n",
        "      elif datetime.strptime(data[-1]['Ngay'], \"%d/%m/%Y\") <= datetime.strptime(company_date[-1], \"%d/%m/%Y\"):\n",
        "        finished = True\n",
        "      else:\n",
        "        index += 1\n",
        "    price_len = len(company_news[company]['price'])\n",
        "    date_len = len(company_date)\n",
        "    if price_len < date_len:\n",
        "        last_price = company_news[company]['price'][-1] if price_len > 0 else 0\n",
        "        company_news[company]['price'].extend([last_price] * (date_len - price_len))"
      ],
      "metadata": {
        "id": "m0EBkCuCHj9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for company in company_list:\n",
        "    print(f\"{company}: {company_news[company]['news']}\")\n",
        "    print(f\"Number of price: {len(company_news[company]['price'])}\")\n",
        "    print(f\"Number of news: {len(company_news[company]['news'])}\")"
      ],
      "metadata": {
        "id": "yFpXQsPqppLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess data"
      ],
      "metadata": {
        "id": "eqMl5woJz6ZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize Vietnamese"
      ],
      "metadata": {
        "id": "OetBpr_KF9ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/stopwords/vietnamese-stopwords/master/vietnamese-stopwords.txt"
      ],
      "metadata": {
        "id": "NxKS4KLOby7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set()\n",
        "with open('vietnamese-stopwords.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        stop_words.add(line.strip())"
      ],
      "metadata": {
        "id": "Y3rIO6pcti92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower() # lowercase text\n",
        "\n",
        "    text = re.sub(r'([a-z]+?)\\1+',r'\\1', text) # reduce repeated character (e.g. 'aaabbb' -> 'ab')\n",
        "\n",
        "    # Ensure space before and after any punctuation mark\n",
        "    text = re.sub(r\"(\\w)\\s*([\" + string.punctuation + \"])\\s*(\\w)\", r\"\\1 \\2 \\3\", text)\n",
        "    text = re.sub(r\"(\\w)\\s*([\" + string.punctuation + \"])\", r\"\\1 \\2\", text)\n",
        "\n",
        "    text = re.sub(f\"([{string.punctuation}])([{string.punctuation}])+\",r\"\\1\", text) # reduce consecutive punctuation\n",
        "\n",
        "    # Remove any leading or trailing spaces, or leading or trailing punctuation marks from the text\n",
        "    text = text.strip()\n",
        "    while text.endswith(tuple(string.punctuation+string.whitespace)):\n",
        "        text = text[:-1]\n",
        "    while text.startswith(tuple(string.punctuation+string.whitespace)):\n",
        "        text = text[1:]\n",
        "\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)) # remove all punctuation\n",
        "\n",
        "    text = re.sub(r\"\\s+\", \" \", text) # reduce multiple spaces\n",
        "\n",
        "    text = text_normalize(text) # make sure punctunation is in the right letter (Vietnamese case)\n",
        "    text = word_tokenize(text, format=\"text\") # tokenize the cleaned text\n",
        "    # text = unidecode(text) # remove accent marks from sentences (no significant difference when accent marks is removed or kept)\n",
        "\n",
        "    text = text.split(' ')\n",
        "\n",
        "    text = [word for word in text if word not in stop_words] # remove stop words\n",
        "\n",
        "    text = ' '.join(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "HRYUFB8OObgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for company in company_list:\n",
        "    print(f\"{company}: {company_news[company]['news']}\")"
      ],
      "metadata": {
        "id": "YTfYCh4zweCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for company, item in company_news.items():\n",
        "    company_news[company]['tokenized_news'] = []\n",
        "    for title in item['news']:\n",
        "        title = clean_text(title)\n",
        "        company_news[company]['tokenized_news'].append(title)"
      ],
      "metadata": {
        "id": "vdHs0lP2LSsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for company in company_list:\n",
        "    print(f\"{company}: {company_news[company]['news']}\")\n",
        "    print(f\"Tokenized news: {company_news[company]['tokenized_news']}\")\n",
        "    print(f\"Number of news: {len(company_news[company]['news'])}\")"
      ],
      "metadata": {
        "id": "jnQH_HR3xAWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot word cloud"
      ],
      "metadata": {
        "id": "8tLK20Ew4eZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://upload.wikimedia.org/wikipedia/commons/f/f2/Logo_Twitter.png"
      ],
      "metadata": {
        "id": "N-MIXjrcBlzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = np.array(Image.open('/content/Logo_Twitter.png'))\n",
        "\n",
        "titles = []\n",
        "for company, item in company_news.items():\n",
        "    titles.extend(item['tokenized_news'])\n",
        "\n",
        "titles = ' '.join(titles)\n",
        "\n",
        "# Create WordCloud object\n",
        "wordcloud = WordCloud(background_color='white',\n",
        "                    mask=mask,\n",
        "                    max_font_size=50,\n",
        "                    contour_width=1,\n",
        "                    contour_color='steelblue',\n",
        "                    min_font_size=10)\n",
        "\n",
        "# Generate word cloud\n",
        "wordcloud.generate(titles)\n",
        "\n",
        "# Plot the WordCloud image\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5tmjLYm69gQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot news length"
      ],
      "metadata": {
        "id": "wzU2nvlkFiTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "company_names = list(company_news.keys())\n",
        "# Tạo danh sách chứa 3 độ dài dài nhất của tokenized_news cho mỗi công ty\n",
        "top_lengths = []\n",
        "for company in company_names:\n",
        "    tokenized_news_lengths = [len(item) for item in company_news[company]['tokenized_news']]\n",
        "    top_lengths.append(sorted(tokenized_news_lengths, reverse=True)[:3])\n",
        "\n",
        "# Vẽ biểu đồ đường\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Tạo mảng các index cho từng công ty\n",
        "index = np.arange(1, len(company_names) + 1)\n",
        "\n",
        "# Vẽ biểu đồ đường cho từng độ dài dài nhất\n",
        "plt.plot(index, [max(lengths) for lengths in top_lengths], marker='o', linestyle='-', color='skyblue', label='Độ dài lớn nhất')\n",
        "plt.plot(index, [sorted(lengths, reverse=True)[1] if len(lengths) > 1 else 0 for lengths in top_lengths], marker='o', linestyle='-', color='orange', label='Độ dài lớn thứ hai')\n",
        "plt.plot(index, [sorted(lengths, reverse=True)[2] if len(lengths) > 2 else 0 for lengths in top_lengths], marker='o', linestyle='-', color='green', label='Độ dài lớn thứ ba')\n",
        "\n",
        "plt.title('Phân bố độ dài tin tức của từng công ty')\n",
        "plt.xlabel('Công ty')\n",
        "plt.ylabel('Số lượng từ')\n",
        "plt.xticks(index, company_names, rotation=45, ha='right')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I1Ke2664FhT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use model VADER from NLTK"
      ],
      "metadata": {
        "id": "td_A4Bls9qD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model"
      ],
      "metadata": {
        "id": "m3kHC6MCQG8E"
      }
    },
    {
      "source": [
        "analyzer = SentimentIntensityAnalyzer()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8DpBGleB4lCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "for company, item in company_news.items():\n",
        "    company_news[company]['sentiment'] = []\n",
        "    for title in item['tokenized_news']:\n",
        "      sentiment = analyzer.polarity_scores(title)\n",
        "      company_news[company]['sentiment'].append(sentiment['compound'])\n",
        "      print(f\"{company}: {title} - {sentiment}\")\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "\n",
        "vader_time_execution = stop - start"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NcWtnP1Y4n0y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for company, item in company_news.items():\n",
        "  print(f\"{company}: {item}\")\n",
        "  print(f\"Number of news: {len(item['news'])}\")\n",
        "  print(f\"Number of timeline: {len(item['timeline'])}\")\n",
        "  print(f\"Number of price: {len(item['price'])}\")"
      ],
      "metadata": {
        "id": "pxRVIi7szMWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_price = []\n",
        "for company, item in company_news.items():\n",
        "  stock_price.append(item['price'])"
      ],
      "metadata": {
        "id": "0lzxxXNQ9IFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "date_time = []\n",
        "for company, item in company_news.items():\n",
        "  date_time.append(item['timeline'])"
      ],
      "metadata": {
        "id": "Nbf_kmuj-2Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot sentiment score"
      ],
      "metadata": {
        "id": "31BoYKFV5vOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vader_sentiment_scores = []"
      ],
      "metadata": {
        "id": "mR_22_JqyJFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(30, 14))\n",
        "\n",
        "for company, item in company_news.items():\n",
        "    date = item['timeline']\n",
        "    price = item['price']\n",
        "    sentiment_scores = item['sentiment']\n",
        "    plot_index = company_list.index(company) + 1\n",
        "    plot_col = 3\n",
        "    plot_row = (len(company_list) + plot_col - 1) // plot_col\n",
        "    ax = fig.add_subplot(plot_row, plot_col, plot_index, frameon=False)\n",
        "    ax.grid(True)\n",
        "    ax.set_title(company)\n",
        "    ax.set_ylim(-1, 1)\n",
        "    dates = pd.to_datetime(date, dayfirst=True)\n",
        "    ax.plot(dates, sentiment_scores, color='c', label='Sentiment score')\n",
        "    ax.legend(loc='upper left')\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "    axt = ax.twinx()\n",
        "    axt.plot(dates, price, color='m', label='Closed price')\n",
        "    axt.legend(loc='upper right')\n",
        "    axt.set_ylim(min(price) - 10, max(price) + 10)\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "    vader_sentiment_scores.append(sentiment_scores)\n",
        "\n",
        "fig.suptitle('VADER sentiment scores')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('vader_sentiment_scores.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WN81grdEkKIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot volatility"
      ],
      "metadata": {
        "id": "Cdai_NVn5yUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(18, 4))\n",
        "volatility_plot = {}\n",
        "\n",
        "for company, item in company_news.items():\n",
        "    sentiment_scores = item['sentiment']\n",
        "    volatility_plot[company] = np.std(sentiment_scores)\n",
        "\n",
        "for company, volatility in volatility_plot.items():\n",
        "    plt.text(company_list.index(company), volatility, f\"{volatility:.2f}\", ha='center', va='bottom')\n",
        "\n",
        "ax.bar(company_list, volatility_plot.values(), 0.3, color='w', edgecolor='k')\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "fig.suptitle('VADER volatility')\n",
        "plt.savefig('vader_volatility.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7kNRU67du_bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use PhoBERT pretrained model from Wonrax"
      ],
      "metadata": {
        "id": "waPCZOJx99h_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model"
      ],
      "metadata": {
        "id": "2HxWJNZpPuWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wonrax = RobertaForSequenceClassification.from_pretrained(\"wonrax/phobert-base-vietnamese-sentiment\")\n",
        "\n",
        "wonrax_tokenizer = AutoTokenizer.from_pretrained(\"wonrax/phobert-base-vietnamese-sentiment\", use_fast=False)"
      ],
      "metadata": {
        "id": "N6sYXU604QHv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "sentiments = {}\n",
        "\n",
        "for company, item in company_news.items():\n",
        "    company_news[company]['sentiment'] = []\n",
        "    for title in item['tokenized_news']:\n",
        "        input_ids = torch.tensor([wonrax_tokenizer.encode(title)])\n",
        "        with torch.no_grad():\n",
        "            out = wonrax(input_ids)\n",
        "            sentiments['neg'] = round(out.logits.softmax(dim=-1).tolist()[-1][0], 3)\n",
        "            sentiments['pos'] = round(out.logits.softmax(dim=-1).tolist()[-1][1], 3)\n",
        "            sentiments['neu'] = round(out.logits.softmax(dim=-1).tolist()[-1][2], 3)\n",
        "            sentiments['compound'] = round(sentiments['pos'] - sentiments['neg'], 4)\n",
        "            company_news[company]['sentiment'].append(sentiments['compound'])\n",
        "            print(f\"{company}: {title} - {sentiments}\")\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "\n",
        "phobert_time_execution = stop - start"
      ],
      "metadata": {
        "id": "Nl7YhYFh4a6U",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for company, item in company_news.items():\n",
        "  print(f\"{company}: {item}\")\n",
        "  print(f\"Number of news: {len(item['news'])}\")\n",
        "  print(f\"Number of timeline: {len(item['timeline'])}\")\n",
        "  print(f\"Number of price: {len(item['price'])}\")"
      ],
      "metadata": {
        "id": "fVCJFZXYzmDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot sentiment score"
      ],
      "metadata": {
        "id": "VNUKx64G-pq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phobert_sentiment_scores = []"
      ],
      "metadata": {
        "id": "zeuWFVYeyk-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(30, 14))\n",
        "\n",
        "for company, item in company_news.items():\n",
        "    date = item['timeline']\n",
        "    price = item['price']\n",
        "    sentiment_scores = item['sentiment']\n",
        "    plot_index = company_list.index(company) + 1\n",
        "    plot_col = 3\n",
        "    plot_row = (len(company_list) + plot_col - 1) // plot_col\n",
        "    ax = fig.add_subplot(plot_row, plot_col, plot_index, frameon=False)\n",
        "    ax.grid(True)\n",
        "    ax.set_title(company)\n",
        "    ax.set_ylim(-1, 1)\n",
        "    dates = pd.to_datetime(date, dayfirst=True)\n",
        "    ax.plot(dates, sentiment_scores, color='c', label='Sentiment score')\n",
        "    ax.legend(loc='upper left')\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "    axt = ax.twinx()\n",
        "    axt.plot(dates, price, color='m', label='Closed price')\n",
        "    axt.legend(loc='upper right')\n",
        "    axt.set_ylim(min(price) - 10, max(price) + 10)\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "    phobert_sentiment_scores.append(sentiment_scores)\n",
        "\n",
        "fig.suptitle('PHOBERT sentiment scores')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('phobert_sentiment_scores.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qPqzFURsBrhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot volatility"
      ],
      "metadata": {
        "id": "xeDJMGVY-vbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(18, 4))\n",
        "volatility_plot = {}\n",
        "\n",
        "for company, item in company_news.items():\n",
        "    sentiment_scores = item['sentiment']\n",
        "    volatility_plot[company] = np.std(sentiment_scores)\n",
        "\n",
        "for company, volatility in volatility_plot.items():\n",
        "    plt.text(company_list.index(company), volatility, f\"{volatility:.2f}\", ha='center', va='bottom')\n",
        "\n",
        "ax.bar(company_list, volatility_plot.values(), 0.3, color='w', edgecolor='k')\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "fig.suptitle('PHOBERT volatility')\n",
        "plt.savefig('phobert_volatility.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_KGnqKjcBwRk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use ViSoBERT"
      ],
      "metadata": {
        "id": "0QgvVEY4ZLFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model"
      ],
      "metadata": {
        "id": "zo2p9eVAZuMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uit = AutoModelForSequenceClassification.from_pretrained('uitnlp/visobert')\n",
        "uit_tokenizer = AutoTokenizer.from_pretrained('uitnlp/visobert')"
      ],
      "metadata": {
        "id": "c4K2ELnoZwkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "sentiments = {}\n",
        "\n",
        "for company, item in company_news.items():\n",
        "    company_news[company]['sentiment'] = []\n",
        "    for title in item['tokenized_news']:\n",
        "        encoding = uit_tokenizer(title, return_tensors='pt')\n",
        "        with torch.no_grad():\n",
        "            output = uit(**encoding)\n",
        "            sentiments['pos'] = round(output.logits.softmax(dim=-1).tolist()[-1][0], 3)\n",
        "            sentiments['neg'] = round(output.logits.softmax(dim=-1).tolist()[-1][1], 3)\n",
        "            sentiments['compound'] = round(sentiments['pos'] - sentiments['neg'], 4)\n",
        "            company_news[company]['sentiment'].append(sentiments['compound'])\n",
        "            print(f\"{company}: {title} - {sentiments}\")\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "\n",
        "visobert_time_execution = stop - start"
      ],
      "metadata": {
        "id": "ZHKV1kOqZKeO",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot sentiment score"
      ],
      "metadata": {
        "id": "JGL9CdEHZ3S-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visobert_sentiment_scores = []"
      ],
      "metadata": {
        "id": "EcPS4JLhysOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(30, 14))\n",
        "\n",
        "for company, item in company_news.items():\n",
        "    date = item['timeline']\n",
        "    price = item['price']\n",
        "    sentiment_scores = item['sentiment']\n",
        "    plot_index = company_list.index(company) + 1\n",
        "    plot_col = 3\n",
        "    plot_row = (len(company_list) + plot_col - 1) // plot_col\n",
        "    ax = fig.add_subplot(plot_row, plot_col, plot_index, frameon=False)\n",
        "    ax.grid(True)\n",
        "    ax.set_title(company)\n",
        "    ax.set_ylim(-1, 1)\n",
        "    dates = pd.to_datetime(date, dayfirst=True)\n",
        "    ax.plot(dates, sentiment_scores, color='c', label='Sentiment score')\n",
        "    ax.legend(loc='upper left')\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "    axt = ax.twinx()\n",
        "    axt.plot(dates, price, color='m', label='Closed price')\n",
        "    axt.legend(loc='upper right')\n",
        "    axt.set_ylim(min(price) - 10, max(price) + 10)\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "    visobert_sentiment_scores.append(sentiment_scores)\n",
        "\n",
        "fig.suptitle('VISOBERT sentiment scores')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('visobert_sentiment_scores.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jGJd6NiMaGMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot volatility"
      ],
      "metadata": {
        "id": "UL-r9os0aGuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(18, 4))\n",
        "volatility_plot = {}\n",
        "\n",
        "for company, item in company_news.items():\n",
        "    sentiment_scores = item['sentiment']\n",
        "    volatility_plot[company] = np.std(sentiment_scores)\n",
        "\n",
        "for company, volatility in volatility_plot.items():\n",
        "    plt.text(company_list.index(company), volatility, f\"{volatility:.2f}\", ha='center', va='bottom')\n",
        "\n",
        "ax.bar(company_list, volatility_plot.values(), 0.3, color='w', edgecolor='k')\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "fig.suptitle('VISOBERT volatility')\n",
        "plt.savefig('visobert_volatility.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BdJCB744aftl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare 3 models"
      ],
      "metadata": {
        "id": "xP3Qq1K7ahI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'tươi'\n",
        "\n",
        "wonrax_sentiment = {}\n",
        "\n",
        "input_ids = torch.tensor([wonrax_tokenizer.encode(sentence)])\n",
        "with torch.no_grad():\n",
        "  out = wonrax(input_ids)\n",
        "  wonrax_sentiment['neg'] = round(out.logits.softmax(dim=-1).tolist()[-1][0], 3)\n",
        "  wonrax_sentiment['pos'] = round(out.logits.softmax(dim=-1).tolist()[-1][1], 3)\n",
        "  wonrax_sentiment['neu'] = round(out.logits.softmax(dim=-1).tolist()[-1][2], 3)\n",
        "  wonrax_sentiment['compound'] = round(wonrax_sentiment['pos'] - wonrax_sentiment['neg'], 4)\n",
        "  print(\"PHOBERT\", wonrax_sentiment)\n",
        "\n",
        "uit_sentiment = {}\n",
        "encoding = uit_tokenizer(sentence, return_tensors='pt')\n",
        "with torch.no_grad():\n",
        "  output = uit(**encoding)\n",
        "  uit_sentiment['pos'] = round(output.logits.softmax(dim=-1).tolist()[-1][0], 3)\n",
        "  uit_sentiment['neg'] = round(output.logits.softmax(dim=-1).tolist()[-1][1], 3)\n",
        "  uit_sentiment['compound'] = round(uit_sentiment['pos'] - uit_sentiment['neg'], 4)\n",
        "  print(\"VISOBERT\", uit_sentiment)\n",
        "\n",
        "print(\"VADER\", analyzer.polarity_scores(sentence))"
      ],
      "metadata": {
        "id": "WQ8Sg_r6b4yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(30, 14))\n",
        "\n",
        "max_length = len(company_news[company_list[0]]['timeline'])\n",
        "\n",
        "for idx, company in enumerate(company_list):\n",
        "    date = date_time[idx]\n",
        "    plot_index = idx + 1\n",
        "    plot_col = 3\n",
        "    plot_row = len(company_list) // plot_col\n",
        "    ax = fig.add_subplot(plot_row, plot_col, plot_index, frameon=False)\n",
        "    ax.grid(True)\n",
        "    ax.set_title(company)\n",
        "    ax.set_ylim(-1, 1)\n",
        "    dates = pd.to_datetime(date, dayfirst=True)\n",
        "    ax.plot(dates, vader_sentiment_scores[idx], color='c', label='VADER')\n",
        "    ax.plot(dates, phobert_sentiment_scores[idx], color='g', label='PHOBERT')\n",
        "    ax.plot(dates, visobert_sentiment_scores[idx], color='y', label='VISOBERT')\n",
        "    ax.legend(loc='upper left')\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "    axt = ax.twinx()\n",
        "    axt.plot(dates, stock_price[idx], color='m', label='Closed price')\n",
        "    axt.legend(loc='upper right')\n",
        "    axt.set_ylim(min(stock_price[idx]) - 10, max(stock_price[idx]) + 10)\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "\n",
        "fig.suptitle('Sentiment scores comparison')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('compare_sentiment_scores.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "odbPgxhrw8yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_execution = {\n",
        "    'VADER': vader_time_execution,\n",
        "    'PHOBERT': phobert_time_execution,\n",
        "    'VISOBERT': visobert_time_execution\n",
        " }\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.yaxis.grid(zorder=0)\n",
        "ax.bar(time_execution.keys(), time_execution.values(), color='lime', width=0.3, linewidth=2.5, zorder=3)\n",
        "ax.set_title('Time execution')\n",
        "ax.set_xlabel('Model')\n",
        "ax.set_ylabel('Time (seconds)')\n",
        "plt.savefig('compare_time_execution.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ekpj_wEe4RFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download file .png"
      ],
      "metadata": {
        "id": "GYG7jF_F4Ov7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# import os\n",
        "\n",
        "# for filename in os.listdir('/content'):\n",
        "#     if filename.endswith('.png'):\n",
        "#         files.download(filename)"
      ],
      "metadata": {
        "id": "P44gtRjKy8CT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
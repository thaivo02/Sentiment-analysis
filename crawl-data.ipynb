{"cells":[{"cell_type":"markdown","metadata":{"id":"ZBlpALwIuqcY"},"source":["# Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T08:01:38.019111Z","iopub.status.busy":"2024-07-07T08:01:38.018639Z","iopub.status.idle":"2024-07-07T08:01:39.937536Z","shell.execute_reply":"2024-07-07T08:01:39.936367Z","shell.execute_reply.started":"2024-07-07T08:01:38.019067Z"},"id":"fP0YXG05b6OA","trusted":true},"outputs":[],"source":["import pandas as pd\n","from bs4 import BeautifulSoup\n","from curl_cffi import requests as curl_requests\n","from datetime import datetime\n","from pymongo.mongo_client import MongoClient\n","from pymongo.server_api import ServerApi"]},{"cell_type":"markdown","metadata":{},"source":["# Connect to MongoDB"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T08:01:39.940901Z","iopub.status.busy":"2024-07-07T08:01:39.940351Z","iopub.status.idle":"2024-07-07T08:01:40.344202Z","shell.execute_reply":"2024-07-07T08:01:40.343174Z","shell.execute_reply.started":"2024-07-07T08:01:39.940868Z"},"trusted":true},"outputs":[],"source":["from requests import get\n","ip = get('https://api.ipify.org').content.decode('utf8')\n","print(ip)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T08:01:40.345995Z","iopub.status.busy":"2024-07-07T08:01:40.345640Z","iopub.status.idle":"2024-07-07T08:02:10.449462Z","shell.execute_reply":"2024-07-07T08:02:10.447732Z","shell.execute_reply.started":"2024-07-07T08:01:40.345966Z"},"trusted":true},"outputs":[],"source":["uri = \"mongodb+srv://mongodb:lPCsg7TdVyNPSMU9@cluster0.e46mp6n.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n","client = MongoClient(uri, server_api=ServerApi('1'))\n","\n","# Send a ping to confirm a successful connection\n","try:\n","    client.admin.command('ping')\n","    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n","except Exception as e:\n","    print(e)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T08:02:10.451125Z","iopub.status.busy":"2024-07-07T08:02:10.450798Z","iopub.status.idle":"2024-07-07T08:02:10.456604Z","shell.execute_reply":"2024-07-07T08:02:10.455351Z","shell.execute_reply.started":"2024-07-07T08:02:10.451096Z"},"trusted":true},"outputs":[],"source":["db = client.get_database(\"stock_company\")"]},{"cell_type":"markdown","metadata":{},"source":["## Store data to MongoDB"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def save_data_to_mongo(data, collection):\n","    try:\n","        collection.insert_many(data)\n","        print(\"Data saved successfully\")\n","    except Exception as e:\n","        print(e)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def convert_data_to_document(data, company, source):\n","    list_timeline = data[\"timeline\"]\n","    list_link = data[\"link\"]\n","    list_title = data[\"title\"]\n","    list_content = data[\"content\"]\n","    list_price = data[\"price\"]\n","    contain_document = list()\n","    for timeline in list_timeline:\n","        document = dict()\n","        document[timeline] = {\n","            \"title\": list_title[list_timeline.index(timeline)] if len(list_title) > list_timeline.index(timeline) else \"No title\",\n","            \"content\": list_content[list_timeline.index(timeline)] if len(list_content) > list_timeline.index(timeline) else \"No content\",\n","            \"url\": list_link[list_timeline.index(timeline)] if len(list_link) > list_timeline.index(timeline) else \"No url\",\n","            \"closed price\": list_price[list_timeline.index(timeline)] if len(list_price) > list_timeline.index(timeline) else \"No price\",\n","            \"source\": source,\n","        }\n","        contain_document.append(document)\n","    return contain_document"]},{"cell_type":"markdown","metadata":{"id":"M6mYVTYiu7SB"},"source":["# Prepare data"]},{"cell_type":"markdown","metadata":{"id":"3t-GSXSAjnzA"},"source":["## Crawl data"]},{"cell_type":"markdown","metadata":{"id":"vQO5Oav4C1n1"},"source":["### Define stock exchange, headers"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T08:02:10.470449Z","iopub.status.busy":"2024-07-07T08:02:10.470117Z","iopub.status.idle":"2024-07-07T08:02:10.482571Z","shell.execute_reply":"2024-07-07T08:02:10.481010Z","shell.execute_reply.started":"2024-07-07T08:02:10.470416Z"},"id":"MzzTcXSCpQXg","trusted":true},"outputs":[],"source":["stock_exchange = ['HOSE', 'UPCOM', 'HNX']\n","stock_exchange.sort()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T05:36:56.199462Z","iopub.status.busy":"2024-07-07T05:36:56.197964Z","iopub.status.idle":"2024-07-07T05:36:56.205692Z","shell.execute_reply":"2024-07-07T05:36:56.204076Z","shell.execute_reply.started":"2024-07-07T05:36:56.199400Z"},"id":"Os2C1FjFcU0N","trusted":true},"outputs":[],"source":["headers={\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 OPR/109.0.0.0\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T05:36:56.782250Z","iopub.status.busy":"2024-07-07T05:36:56.781801Z","iopub.status.idle":"2024-07-07T05:36:56.788319Z","shell.execute_reply":"2024-07-07T05:36:56.786931Z","shell.execute_reply.started":"2024-07-07T05:36:56.782190Z"},"id":"Ae3EhoSKjunh","trusted":true},"outputs":[],"source":["ssi_url = \"https://iboard-query.ssi.com.vn/v2/stock/exchange/\"\n","ssi_iboard_url = f\"https://iboard-api.ssi.com.vn/statistics/company/stock-price\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T05:37:04.902665Z","iopub.status.busy":"2024-07-07T05:37:04.902129Z","iopub.status.idle":"2024-07-07T05:37:04.909165Z","shell.execute_reply":"2024-07-07T05:37:04.907656Z","shell.execute_reply.started":"2024-07-07T05:37:04.902626Z"},"id":"QikxSJiDkpHR","trusted":true},"outputs":[],"source":["fromDate = '07/07/2019'\n","toDate = '07/07/2024'"]},{"cell_type":"markdown","metadata":{"id":"WVIJnvgzkq14"},"source":["### Crawl more company data from SSI"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T05:37:07.150982Z","iopub.status.busy":"2024-07-07T05:37:07.150549Z","iopub.status.idle":"2024-07-07T05:37:07.159964Z","shell.execute_reply":"2024-07-07T05:37:07.158310Z","shell.execute_reply.started":"2024-07-07T05:37:07.150949Z"},"id":"w0lc46Z0ku8p","trusted":true},"outputs":[],"source":["def get_company_list(company_list, stock_exchange):\n","    for exchange in stock_exchange:\n","        url = f\"{ssi_url}{exchange}\"\n","        print(url)\n","        response = curl_requests.get(url, headers = headers)\n","        if response.status_code == 200:\n","            response = response.json()\n","            if 'data' in response:\n","                data = response['data']\n","                company_list.extend(company['ss'].upper() for company in data if company['ss'].upper() not in company_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T05:37:09.312969Z","iopub.status.busy":"2024-07-07T05:37:09.312496Z","iopub.status.idle":"2024-07-07T05:37:11.999968Z","shell.execute_reply":"2024-07-07T05:37:11.998336Z","shell.execute_reply.started":"2024-07-07T05:37:09.312935Z"},"id":"zuLBJ5GZoTZN","outputId":"fcc61f10-4309-4f9b-c28a-afc8496d6a81","trusted":true},"outputs":[],"source":["company_list = []\n","get_company_list(company_list, stock_exchange)\n","company_list.sort()\n","print(f\"Number of company: {len(company_list)}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Remove existed company in database"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["existed_company = list(db.list_collections())\n","existed_company = [company['name'] for company in existed_company]\n","existed_company.sort()\n","company_list = [company for company in company_list if company not in existed_company]"]},{"cell_type":"markdown","metadata":{"id":"UNYY86dZjmBg"},"source":["### Define crawler function"]},{"cell_type":"markdown","metadata":{},"source":["## Get, update company closed price"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def update_price(timeline, list_price, ssi_date, close_price):\n","    try:\n","        if ssi_date in timeline:\n","            index = timeline.index(ssi_date)\n","            list_price[index] = float(close_price)\n","    except Exception:\n","        pass"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def duplicate_price(timeline, list_price):\n","    price_len = len(list_price)\n","    timeline_len = len(timeline)\n","    if price_len < timeline_len:\n","        last_price = list_price[-1] if price_len > 0 else 0\n","        list_price.extend([last_price] * (timeline_len - price_len))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m0EBkCuCHj9r","trusted":true},"outputs":[],"source":["def get_company_price(company, fromDate, toDate, vietstock_news, cafef_news):\n","    vietstock_timeline = list(vietstock_news[company]['timeline'])\n","    vietstock_price = list(0 for _ in range(len(vietstock_timeline)))\n","    cafef_timeline = list(cafef_news[company]['timeline'])\n","    cafef_price = list(0 for _ in range(len(cafef_timeline)))\n","    # startDate = list_timeline[-1]\n","    # startDate = datetime.strptime(startDate, \"%d/%m/%Y\").strftime(\"%d%%2F%m%%2F%Y\")\n","    # endDate = list_timeline[0]\n","    # endDate = datetime.strptime(endDate, \"%d/%m/%Y\").strftime(\"%d%%2F%m%%2F%Y\")\n","    finished = False\n","    index = 1\n","    max_index = 1\n","    while not finished:\n","        # url = f\"https://s.cafef.vn/Ajax/PageNew/DataHistory/PriceHistory.ashx?Symbol={company}&StartDate={startDate}&EndDate={endDate}&PageIndex={index}\"\n","        url = f\"{ssi_iboard_url}?symbol={company}&page={index}&pageSize=10&fromDate={fromDate}&toDate={toDate}\"\n","        try:\n","            response = curl_requests.get(url, headers = headers, timeout = 10)\n","            if response.status_code == 200:\n","                data = response.json()\n","                total = data['paging']['total']\n","                max_index = (total + 9) // 10\n","                data = data.get('data', [])\n","                for item in data:\n","                    ssi_date = item['tradingDate'].split(' ')[0]\n","                    close_price = item['closePrice']\n","                    update_price(vietstock_timeline, vietstock_price, ssi_date, close_price)\n","                    update_price(cafef_timeline, cafef_price, ssi_date, close_price)\n","                print(f\"Fetched price data on page: {index}\")\n","            else:\n","                print(f\"Failed to fetch price data for {company}\")\n","            if index == max_index:\n","                finished = True\n","            else:\n","                index += 1\n","        except Exception as e:\n","            print(f\"Request failed for {company}. Skipping to the next page.\")\n","            index += 1\n","            pass\n","    duplicate_price(vietstock_timeline, vietstock_price)\n","    duplicate_price(cafef_timeline, cafef_price)\n","    vietstock_news[company]['price'] = vietstock_price\n","    cafef_news[company]['price'] = cafef_price"]},{"cell_type":"markdown","metadata":{},"source":["## Get content from title link"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_content(url, source):\n","    try:\n","        contents = list()\n","        response = curl_requests.get(url, headers=headers, timeout=10)\n","        if response.status_code == 200:\n","            body = BeautifulSoup(response.content, \"html.parser\")\n","            if source == \"cafef\":\n","                contents = body.find_all(\"div\", {\"id\": \"newscontent\"})\n","            else:\n","                contents = body.find_all(\"div\", {\"id\": \"vst_detail\"})\n","            contents = [content.text.strip() for content in contents]\n","    except Exception as e:\n","        print(f\"Error occurred: {str(e)}\")\n","        print(f\"Request timed out for link: {url}.\")\n","    return contents"]},{"cell_type":"markdown","metadata":{},"source":["## Get title, time written and content url"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W2703k-vxVSU","trusted":true},"outputs":[],"source":["def get_cafef_news(company, fromDate, toDate, site_url):\n","    index = 1\n","    finished = False\n","    list_title = list()\n","    list_timeline = list()\n","    list_link = list()\n","    while not finished:\n","        url = f\"{site_url}?symbol={company}&floorID=0&configID=0&PageIndex={index}&PageSize=30&Type=2\"\n","        try:\n","            response = curl_requests.get(url, headers=headers, timeout=10)\n","            if response.status_code == 200:\n","                body = BeautifulSoup(response.content, \"html.parser\")\n","                titles = body.find_all(\"a\", class_=\"docnhanhTitle\")\n","                titles = [title.text.strip().split(\": \")[-1] for title in titles]\n","                timelines = body.find_all(\"span\", class_=\"timeTitle\")\n","                timelines = [time.text.strip().split(\" \")[0] for time in timelines]\n","                if (\n","                    len(titles) != len(timelines)\n","                    or len(titles) == 0\n","                    or len(timelines) == 0\n","                ):\n","                    raise Exception({\"message\": \"Data not found\", \"code\": 404})\n","                list_title.extend(titles)\n","                list_timeline.extend(timelines)\n","                links = body.find_all(\"a\")\n","                links = [link[\"href\"] for link in links]\n","                list_link.extend(links)\n","                print(f\"Fetched data on page: {index}\")\n","            else:\n","                print(f\"Failed to fetch data for {company} on page: {index}\")\n","            finished = datetime.strptime(\n","                timelines[-1], \"%d/%m/%Y\"\n","            ) <= datetime.strptime(fromDate, \"%d/%m/%Y\")\n","            if not finished:\n","                index += 1\n","        except Exception as e:\n","            code = e.args[0][\"code\"]\n","            print(f\"Data not found for {company} on page: {index}\" if code == 404 else f\"Request timed out for {company}. Skipping to the next page.\")\n","            finished, index = (True, index) if code == 404 else (finished, index + 1)\n","            pass\n","    return list_title, list_timeline, list_link"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ie3KNo1JkUAB","trusted":true},"outputs":[],"source":["def get_vietstock_news(company, fromDate, toDate, site_url, company_news):\n","    index = 1\n","    max_index = 1\n","    finished = False\n","    list_title = list()\n","    list_timeline = list()\n","    list_link = list()\n","    while not finished:\n","        data = {\n","            \"view\": \"1\",\n","            \"code\": company,\n","            \"type\": \"1\",\n","            \"fromDate\": fromDate,\n","            \"toDate\": toDate,\n","            \"channelID\": \"-1\",\n","            \"page\": index,\n","            \"pageSize\": \"20\",\n","        }\n","        url = site_url\n","        try:\n","            # Create a copy of headers and modify the copy\n","            request_headers = headers.copy()\n","            request_headers[\"Content-Type\"] = \"application/x-www-form-urlencoded\"\n","            response = curl_requests.post(\n","                url, headers=request_headers, data=data, timeout=10\n","            )\n","            if response.status_code == 200:\n","                body = BeautifulSoup(response.content, \"html.parser\")\n","                total_pages = body.find_all(\"div\", class_=\"m-b pull-left\")\n","                if len(total_pages) > 0:\n","                    total_pages = int(total_pages[-1].text.split(\" \")[-1])\n","                    max_index = total_pages\n","                titles = body.find_all(\"a\", class_=\"text-link news-link\")\n","                titles = [title.text.strip().split(\": \")[-1] for title in titles]\n","                timelines = body.find_all(\"td\", class_=\"col-date\")\n","                timelines = [time.text.strip().split(\" \")[0] for time in timelines]\n","                if (\n","                    len(titles) != len(timelines)\n","                    or len(titles) == 0\n","                    or len(timelines) == 0\n","                ):\n","                    raise Exception({\"message\": \"Data not found\", \"code\": 404})\n","                list_title.extend(titles)\n","                list_timeline.extend(timelines)\n","                links = body.find_all(\"a\", class_=\"text-link news-link\")\n","                links = [link[\"href\"] for link in links]\n","                list_link.extend(links)\n","                print(f\"Fetched data on page: {index}\")\n","            else:\n","                print(f\"Failed to fetch data for {company} on page: {index}\")\n","            index += 1\n","            finished = index > max_index\n","        except Exception as e:\n","            code = e.args[0][\"code\"]\n","            print(f\"Data not found for {company} on page: {index}\" if code == 404 else f\"Request timed out for {company}. Skipping to the next page.\")\n","            finished, index = (True, index) if code == 404 else (finished, index + 1)\n","            pass\n","    return list_title, list_timeline, list_link"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"vT0sgIrQfOHr","trusted":true},"outputs":[],"source":["def export_data_to_csv(company_news, source):\n","    company_news_df = pd.DataFrame(company_news)\n","    company_news_df = company_news_df.T\n","    company_news_df.reset_index(inplace=True)\n","    company_news_df.rename(columns={'index': 'company'}, inplace=True)\n","    company_news_df.to_csv(f\"data/{source}.csv\", index=False)\n","    company_news_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def store_data_to_company_news(company_news, company, list_title, list_timeline, list_link, list_content):\n","    company_news[company] = {}\n","    company_news[company]['title'] = list_title\n","    company_news[company]['timeline'] = list_timeline\n","    company_news[company]['link'] = list_link\n","    company_news[company]['content'] = list_content"]},{"cell_type":"markdown","metadata":{},"source":["### Crawl from CafeF & VietStock"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cafef_base_url = \"s.cafef.vn\"\n","cafef_news = {}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vietstock_base_url = \"finance.vietstock.vn\"\n","vietstock_news = {}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for company in company_list:\n","    print(f\"Fetching data for {company}\")\n","\n","    # Get news data from CafeF\n","    cafef_news[company] = {}\n","    source = \"cafef\"\n","    site_url = f\"https://{cafef_base_url}/Ajax/Events_RelatedNews_New.aspx\"\n","    list_title, list_timeline, list_link = get_cafef_news(\n","        company, fromDate, toDate, site_url\n","    )\n","    list_content = list()\n","    print(f\"Get content for {company}\")\n","    for link in list_link:\n","        content_url = f\"https://{cafef_base_url}{link}\"\n","    list_content.extend(get_content(content_url, source))\n","    store_data_to_company_news(\n","        cafef_news, company, list_title, list_timeline, list_link, list_content\n","    )\n","\n","    # Get news data from Vietstock\n","    vietstock_news[company] = {}\n","    source = \"vietstock\"\n","    site_url = f\"https://{vietstock_base_url}/View/PagingNewsContent\"\n","    list_title, list_timeline, list_link = get_vietstock_news(\n","        company, fromDate, toDate, site_url, vietstock_news\n","    )\n","    list_content = list()\n","    for link in list_link:\n","        content_url = f\"https:{link}\" if not link.startswith(\"https:\") else link\n","        content_url = content_url.replace(\"\\n\", \"\")\n","        list_content.extend(get_content(content_url, source))\n","    store_data_to_company_news(\n","        vietstock_news, company, list_title, list_timeline, list_link, list_content\n","    )\n","\n","    # Get price data\n","    get_company_price(company, fromDate, toDate, vietstock_news, cafef_news)\n","\n","    # Convert data to document and save to MongoDB\n","    cafef_document = convert_data_to_document(cafef_news[company], company, \"cafef\")\n","    vietstock_document = convert_data_to_document(vietstock_news[company], company, \"vietstock\")\n","\n","    # Save data to MongoDB\n","    contain_document = cafef_document + vietstock_document\n","    collection = db.get_collection(company)\n","    save_data_to_mongo(contain_document, collection)\n","    \n","    print(f\"Data fetched successfully for {company}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Export data to CSV"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# Export data to CSV\n","export_data_to_csv(cafef_news, \"cafef\")\n","export_data_to_csv(vietstock_news, \"vietstock\")"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
